---
title: Upgrading Javascript AWS-SDK to v3
categories: '#AWS #DynamoDB #NoSQL #GraphQL #Serverless'
keywords: aws,dynamodb,aws lambda,graphql,dynamodb table,nosql,cloud,amazon web services,dynamodb with graphql,serverless dynamodb
date: '2021-03-14T22:12:03.284Z'
length: 5 minutes
image: https://images.unsplash.com/photo-1580927752452-89d86da3fa0a?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1020&q=80
---

import { CodeWave } from 'gatsby-theme-waves'

In December 2020, [AWS released AWS SDK v3 for Javascript](https://aws.amazon.com/about-aws/whats-new/2020/12/aws-sdk-javascript-version-3-generally-available/). It promises to be modular and have more tree-shakeable structure, resulting in smaller artifact sizes. This is especially important, if you're interacting with AWS resources inside Lambda functions where each KB of code affects your cold-start time and \$\$\$. Unfortunately, migration isn't straightforward. The signature of new V3 calls is new - some functions are gone and some are replaced with new modules.

In this blogpost, I show upgrading strategies from AWS SDK V2 to V3 for a few most common use cases in Serverless, at least for me. Official docs are describing [three migration paths](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/migrating-to-v3.html) but for the purpose of this article, we'll focus only on the 3rd Path - the proper one using the Command objects and fully `async/await` based approach.

### DynamoDB

The most frequent complaint regarding to the new DynamoDB client from AWS SDK V3 was a lack of `DocumentClient` library. Instead, we were forced to call `marshall()` / `unmarshall()` whenever inserting, putting or getting items from DynamoDB. This was introducing a lot of unnecessary complexity and caused code to be harder to follow and test.

Fortunately, [AWS DynamoDB Document Client has been recently added to the V3](https://github.com/aws/aws-sdk-js-v3/pull/2097).

<CodeWave>

```ts
import { DocumentClient } from 'aws-sdk/clients/dynamodb'
const ddb = new DocumentClient()

const TableName = 'my-table'
const Item = {
  id: '1',
  name: 'Dynobase',
}

await ddb
  .put({
    TableName,
    Item,
  })
  .promise()
```

In V2, once we had our `DocumentClient` constructed, interaction with DynamoDB in Node.js was boiling down to using [methods like `get`, `put`, `scan` or `query`](https://dynobase.dev/dynamodb-nodejs/) and ending them with `.promise()` call.

```ts
// New - V3
import { DynamoDBClient } from '@aws-sdk/client-dynamodb'
import { DynamoDBDocumentClient, PutCommand } from '@aws-sdk/lib-dynamodb'

const client = new DynamoDBClient({})
const ddbDocClient = DynamoDBDocumentClient.from(client)

await ddbDocClient.send(
  new PutCommand({
    TableName,
    Item,
  })
)
```

In V3, `DocumentClient` is imported from a different package and created using `DynamoDBDocumentClient.from()` method.
Moreover, you can call the document client operations using [Command objects](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/classes/_aws_sdk_lib_dynamodb.getcommand-1.html).

```ts
import { DynamoDBClient } from '@aws-sdk/client-dynamodb'
import { DynamoDBDocument } from '@aws-sdk/lib-dynamodb'

const client = new DynamoDBClient({})
const ddbDocClient = DynamoDBDocument.from(client)

await ddbDocClient.put({
  TableName,
  Item,
})
```

If you don't like this new style, you can still use old `.put()` method too. However, in this case you need to import `DynamoDBDocument` instead of `DynamoDBDocumentClient`.

</CodeWave>

<br />

### S3

Interacting with Simple Storage from Lambdas is a very common pattern. Let's talk about uploading, downloading and preparing presigned URLs.

#### Uploading files

<CodeWave>

```ts
import { S3 } from 'aws-sdk'
const s3 = new S3()

const bucket = 'my-bucket'
const key = 'cat.png'
const fileContent = fs.readFileSync('path-to-file.jpg')

await s3
  .upload({
    Bucket: bucket,
    Key: key,
    Body: fileContent,
  })
  .promise()
```

The complexity of uploading large files to S3 in V2 is hidden inside `upload` method. It takes care of initiating the multipart upload, uploading parts and calculating checksums. There's also famous `.promise()` call at the end to _promisify_ the call.

```ts
import { S3 } from 'aws-sdk'
const s3 = new S3()

const bucket = 'my-bucket'
const key = 'cat.png'
const fileContent = fs.readFileSync('path-to-file.jpg')

await s3
  .upload({
    Bucket: bucket,
    Key: key,
    Body: fileContent,
  })
  .promise()

// New - V3
import { S3Client } from '@aws-sdk/client-s3'
import { Upload } from '@aws-sdk/lib-storage'

const buffer = fs.readFileSync('path-to-file.jpg')
const multipartUpload = new Upload({
  client: new S3Client({}),
  params: { Bucket: bucket', Key: key, Body: buffer },
})

await multipartUpload.done();
```

In V3, we are not only forced to import SDK in a modular way, but also we have to import two packages: `client-s3` - responsible for communicating with AWS S3, and `lib-storage` which is a high-level helper library containing classes like `Upload` which now take care of multipart upload orchestration.

</CodeWave>

#### Generating presigned URLs

Presigned URLs are useful when you need to expose some data to the client in a secure way or allow them to upload files from the client directly to the S3. In such scenario, client requests backend for a presigned URL, backend sends it back to the client, and the client initiates uploading procedure to the S3 directly using provided URL.

<CodeWave>

```ts
import { S3 } from 'aws-sdk'
const s3 = new S3()

const params = { Bucket: 'bucket', Key: 'key' }
const url = s3.getSignedUrl('getObject', params) // or getSignedUrlPromise
```

In V2, you had two options to generate pre-signed URLs - synchronously and asynchronously.

```ts
import { S3 } from 'aws-sdk'
const s3 = new S3()

const params = { Bucket: 'bucket', Key: 'key' }
const url = s3.getSignedUrl('getObject', params) // or getSignedUrlPromise

// New - V3
import { getSignedUrl } from '@aws-sdk/s3-request-presigner'
import { S3Client, GetObjectCommand } from '@aws-sdk/client-s3'

const client = new S3Client()
const command = new GetObjectCommand(params)
const url = await getSignedUrl(client, command, { expiresIn: 3600 })
```

In V3, there's only one option - asynchronous one. Moreover, it requires you to import `@aws-sdk/s3-request-presigner` package and call `getSignedUrl` using the client and the Command object.

</CodeWave>

### Lambda

#### Invoking functions

Even though [invoking functions from functions is considered as an antipattern](https://twitter.com/hichaelmart/status/1282647858920009728) by some, I believe that's not the case if you know what you're doing.

<CodeWave>

```ts
import { Lambda } from 'aws-sdk'
const lambda = new Lambda()

const params = {
  FunctionName: 'Lambda_B',
  InvocationType: 'RequestResponse',
  LogType: 'Tail',
  Payload: '{ "name" : "Alex" }',
}
await lambda.invoke(params)
```

In V2, invoking Lambda asynchronously or synchronously, required just the `FunctionName`. Optional parameter like `LogType: 'Tail'` are helpful if we want to forward up to 4KB of logs from the invoked Lambda to the caller.

```ts
import { Lambda } from 'aws-sdk'
const lambda = new Lambda()

const params = {
  FunctionName: 'Lambda_B',
  InvocationType: 'RequestResponse',
  LogType: 'Tail',
  Payload: '{ "name" : "Alex" }',
}
await lambda.invoke(params)

// New - V3
import { LambdaClient, InvokeCommand } from '@aws-sdk/client-lambda'
const lambda = new LambdaClient({})

const params = new InvokeCommand({
  FunctionName: 'Lambda_B',
  InvocationType: 'RequestResponse',
  LogType: 'tail',
  Payload: Buffer.from('{ "name" : "Alex" }'), // Uint8Array
})
await lambda.send(params)
```

In V3, signature is almost identical. The difference is in the `Payload` parameter. While V2 accepted `string`, V3 accepts only `Uint8Array`. Since in Node.js `Buffer` instances are also [`Uint8Array` instances](https://nodejs.org/api/buffer.html#buffer_buffers_and_typedarrays), we can use them instead.

</CodeWave>

### Other notes / problems

#### Using SSO-based credentials

Up to version 3.7.0, using SSO credentials was impossible. The workaround was to copy & paste credentials provided by the SSO as environment variables. Luckily, [this issue is now solved](https://github.com/aws/aws-sdk-js-v3/pull/2055).

#### Lack of XRay support

As I'm writing this, AWS SDK V3 is not working correctly with X-Ray which is a showstopper for many of us. This is due to the new middleware architecture which is incompatible with how the X-Ray SDK hooks into the version 2.x. But, there's a pull request already revieved as [☑️, waiting to be merged](https://github.com/aws/aws-xray-sdk-node/pull/386)

#### Lack of `ChainableTemporaryCredentials`

Currently, [AWS SDK V3 is not supporting this](https://github.com/aws/aws-sdk-js-v3/issues/1796). This is especially useful in multi-tenant SaaS environments where your Lambdas/Containers/VMs have to assume a dynamic role based on runtime input/context.

#### Reusing HTTP connections

In V2, there was this famous _Keep-Alive_ trick which forced AWS SDK to [reuse HTTP connections](https://theburningmonk.com/2019/02/lambda-optimization-tip-enable-http-keep-alive/). This reduced the amount of HTTP handshakes performed and improved overall Lambda performance. In V3, this is [happening automatically](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/node-reusing-connections.html).
